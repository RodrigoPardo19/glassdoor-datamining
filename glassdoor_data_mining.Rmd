---
title: "Proyecto Mineria de Datos Glassdoor Jobs"
author: 'Rodrigo Pardo, Jonathan Lizama, Juan Aguilera'
date: 'Fecha: 18-04-2022'
output: 
  html_document:
    number_sections: yes
    theme: paper
    toc: yes
  pdf_document:
    toc: yes
---

# Planteamiento del problema y motivación

En la actualidad la industria del software está creciendo de manera acelerada, la demanda en los distintos puestos de trabajo ha aumentado considerablemente a tal punto que las ofertas de profesionales no pueden satisfacer el ritmo de los nuevos puestos de trabajo en tecnología. Esto aplica no solo a nuestro país que según estimaciones existe una escasez del 30% de puestos de trabajos sin satisfacer, en Europa este fenómeno es aún peor llegando en España a tener 40% y en Alemania 35% de puestos sin suplir y se proyecta que a futuro estas cifras sólo van a aumentar.

Debido a esto es que las empresas han elaborado distintas estrategias para atraer a los profesionales y los nuevos talentos, grandes salarios, buenas condiciones laborales, capacitaciones, beneficios laborales, son solo algunas de las medidas implementadas por estas compañías. A raíz de esto es que se han creado muchos sitios web especializados en reclutamiento para trabajos de tecnología, Glasdoor es una de estos sitios más populares en Europa donde todos los días aparecen nuevas ofertas de trabajos, por lo que para este proyecto haremos uso de un dataset que contiene la toda la información relevante de cada una de estas ofertas, esto nos permitirá realizar una exploración y análisis de estos datos en base a las habilidades requeridas, experiencia, salarios, valoraciones de las empresas, comentarios de los trabajadores o postulantes, beneficios, perfiles, etc. Lo que nos llevará a realizar un estudio de la situación actual de trabajos en tecnología en Europa. Además de comparar los resultados con estudios realizados en Chile y Latinoamérica para establecer las similitudes y diferencias que puedan existir.

# Exploración de Datos

El dataset que se decidió usar fue obtenido de Kaggle, es un conjunto de datos que se creó haciendo web scraping del sitio de reclutamiento Glassdoor, obteniendo más de 160 mil trabajos en el dataset en los cuales se almacenaron atributos como el cargo, la descripción y título de los trabajos, beneficios, compañía, rating, salarios, tiempo, comentarios, etc. Podemos hacernos una pequeña idea del conjunto de datos si analizamos la web de Glassdoor y ver de qué forma está compuesta, ya que los datos fueron extraídos directamente desde el sitio. Se debe considerar esto solo como la primera aproximación para comprender el problema, los datos y su estructura, posterior a esto se realizará una exploración del conjunto de datos para estudiar y comprender mejor el dataset que se analizará.


```{r}
# Paquetes requeridos
library(tidyverse)
library(plyr)
library(dplyr)
library(sjmisc)
library(readr)
library(tm)
library(NLP)
```


```{r}
glassdoor_data <- read.csv("dataset-glassdoor/glassdoor.csv", header = T, encoding = "UTF-8", as.is = F)

```

```{r}
dim(glassdoor_data)

```
Ahora observamos cual es la dimensión del dataset mas grande, que en este caso es glassdoor.csv, el cual tiene una dimensión de: 165290 filas y 163 columnas.

El rango de fecha que abarcan los trabajos que se utilizarán en el dataset es:
```{r}
fechasTrabajosPublicados <- glassdoor_data[order(as.Date(glassdoor_data$header.posted, format="%b %d, %Y")),]
fechasTrabajosPublicados[1, c("header.posted")]
fechasTrabajosPublicados[length(fechasTrabajosPublicados), c("header.posted")]
```


```{r}
tablaEmpresa<-glassdoor_data[,c("gaTrackerData.empName","gaTrackerData.industry","gaTrackerData.jobTitle","overview.size","overview.foundedYear","overview.type","rating.recommendToFriend","rating.starRating","reviews","salary.salaries","header.rating","header.posted")]
names(tablaEmpresa)
```

Las columnas mas importantes podrian ser gaTrackerData.empName que es la empresa que ofrece el empleo, overview.size la canitdad de empleados,gaTrackerData.industry es la industria donde se envuelve la empresa, overview.competitors cantidad de competidores para el puesto, salary.salaries salario del puesto de trabajo, gaTrackerData.jobTitle es el titulo del empleo, entre otras.


```{r}
diferencia_salario<-unique(glassdoor_data$salary.salaries)
diferencia_salario[c(1,147150)]

```

Una informacion que podría ser importante puede ser, la diferencia salarial entre el menor valor que es 2483 y el mayor que es 52079, la moneda que se esta utilizando es la moneda del reino unido.


En este ejercicio se filtraron los Títulos de los trabajos por la posición que se buscaba, sin embargo esta columna no se encotraba normalizada, por lo que por ejemplo si buscabamos por busqueda exacta el puesto de Sofware Engineer, no se muestra el total de puestos para este trabajo puestos que existian por ejemplo Backend Software Engineer los cuales no iban a figurar por búsqueda exacta. Por lo que se recurrió a la función grepl, esta funcion realiza un includes por cada titulo de trabajo reflejando de forma exacta las palabras buscadas. Es por esto que se estableció de forma manual las posiciones a buscar y su número de ofertas de trabajo, obteniendo los siguentes resultados:

```{r}
softwareEngineers <- dplyr::filter(glassdoor_data, grepl("Software Engineer", `header.jobTitle`, ignore.case = TRUE))
productManagers <- dplyr::filter(glassdoor_data, grepl("Product Manager", `header.jobTitle`,  ignore.case = TRUE))
dataScientist <- dplyr::filter(glassdoor_data, grepl("Data Scientist", `header.jobTitle`,  ignore.case = TRUE))
etlEngineers <- dplyr::filter(glassdoor_data, grepl("ETL", `header.jobTitle`,  ignore.case = TRUE))
devopsEngineers <- dplyr::filter(glassdoor_data, grepl("DevOps", `header.jobTitle`,  ignore.case = TRUE))
projectManagers <- dplyr::filter(glassdoor_data, grepl("Project Manager", `header.jobTitle`,  ignore.case = TRUE))
dataEngineers <- dplyr::filter(glassdoor_data, grepl("Data Engineer", `header.jobTitle`,  ignore.case = TRUE))

puestosTrabajos = data.frame("Posición" = c("Software Engineer", "Product Manager", "Data Scientist", "ETL Engineer", "DevOps", "Project Manager", "Data Engineer"), "Cantidad" = c(nrow(softwareEngineers),  nrow(productManagers), nrow(dataScientist), nrow(etlEngineers), nrow(devopsEngineers), nrow(projectManagers), nrow(dataEngineers)))

puestosTrabajosMasDemandados <- puestosTrabajos[order(-puestosTrabajos$Cantidad), ]
puestosTrabajosMasDemandados
```
Como se puede observar la posición de desarrollador de software en diferentes lenguajes y experiencia (junior, senior, etc) es el cargo más demandado junto a project manager, sin embargo también se encontró un fenómeno bastante interesante, resulta que cuando se buscan cargos relaciados a los datos, existe una granuaridad bastante extraña que veremos en el siguiente ejercicio.

```{r}
library(ggplot2)  # cargamos la librería ggplot2 para la vizualización de datos.
vacantes <- puestosTrabajos$Posición
ggplot(puestosTrabajos) +   # asociamos un data frame a ggplot
geom_bar(aes(x = Cantidad , y = vacantes , fill = vacantes), stat="identity") + theme_bw(base_size = 12)+
  ggtitle("Oferta Laboral Diciembre Del 2019 ") + # título
  xlab("Cantidad de oferta") + ylab("Puestos de Trabajo")  # etiquetas
```

Tecnologías más demandadas

```{r}
java <- dplyr::filter(glassdoor_data, grepl("Java", `header.jobTitle`, ignore.case = TRUE))
php <- dplyr::filter(glassdoor_data, grepl("Php", `header.jobTitle`,  ignore.case = TRUE))
python <- dplyr::filter(glassdoor_data, grepl("Python", `header.jobTitle`,  ignore.case = TRUE))
go <- dplyr::filter(glassdoor_data, grepl("Go", `header.jobTitle`,  ignore.case = TRUE))
html <- dplyr::filter(glassdoor_data, grepl("HTML", `header.jobTitle`,  ignore.case = TRUE))
sql <- dplyr::filter(glassdoor_data, grepl("SQL", `header.jobTitle`,  ignore.case = TRUE))
elixir <- dplyr::filter(glassdoor_data, grepl("Elixir", `header.jobTitle`,  ignore.case = TRUE))
ruby <- dplyr::filter(glassdoor_data, grepl("Ruby", `header.jobTitle`,  ignore.case = TRUE))
cloud <- dplyr::filter(glassdoor_data, grepl("Cloud", `header.jobTitle`,  ignore.case = TRUE))
aws <- dplyr::filter(glassdoor_data, grepl("Aws", `header.jobTitle`,  ignore.case = TRUE))

tecnologias = data.frame("Tecnología" = c("Java", "PHP", "Python", "Go", "HTML", "SQL", "Elixir", "Ruby", "Cloud", "AWS"), "Cantidad" = c(nrow(java),  nrow(php), nrow(python), nrow(go), nrow(html), nrow(sql), nrow(elixir), nrow(ruby), nrow(cloud), nrow(aws)))

tecnologiasDemandadas <- tecnologias[order(-tecnologias$Cantidad), ]

tecnologiasDemandadas
```


```{r}
# Para este ejercicios vamos a intentar ver cuantos trabajos relacionados a datos podemos encontrar
dataRelatedJobs <- dplyr::filter(glassdoor_data, grepl("data", `header.jobTitle`, ignore.case = TRUE))
nrow(dataRelatedJobs)
```

Como se puede ver, solamente en trabajos relacionados a datos existen casi 26 mil ofertas, sin embargo como se comentaba anteriormente para estas ofertas ocurre un fenómeno bastante extraño, y es que existen muchos cargos o posiciones distintas en relación a trabajos con datos, de hecho con mostrar solamente una muestra muy pequeña nos podemos dar cuenta de esto:

```{r}
dataRelatedJobs[10:30, c("header.jobTitle")]
```

Data Engineer, Data Scientist, ETL Developer,  Big Data Analyst, Data Quality, Database Administrator, Big Data Engineer, Spec Analitycs, etc, etc...
Es decir cuando se quieren analizar de forma más especificas los cargos relacionados a los datos nos encontramos con una granularidad muy grandes de cargos, esto puede ser más interesante de analizar en un futuro y por lo tanto llegarnos a plantear ciertas preguntas que se responderán en un hito futuro con otro tipo de análisis. ¿Por qué existen tantas granularidades en los cargos relacionados a los datos?, ¿Qué skills/habilidades se requieren para cada tipo de cargo? con estás preguntas  estudiaremos si realmente existe una gran diferencia entre los distintos puestos relacionados a los datos o si realmente existe un desorden en los nombres de cada cargo, ya que realmente las habilidades son distintas pero con otros nombre, es decir averiguaremos si un data engineer es lo mismo que un data scientist o big data analyst, data quality, o por el contrario cuales son sus diferencias.

Es decir cuando se quieren analizar de forma más especificas los cargos relacionados a los datos nos encontramos con una granularidad muy grandes de cargos, esto puede ser más interesante de analizar en un futuro y por lo tanto llegarnos a plantear ciertas preguntas que se responderán en un hito futuro con otro tipo de análisis. ¿Por qué existen tantas granularidades en los cargos relacionados a los datos?, ¿Qué skills/habilidades se requieren para cada tipo de cargo? con estás preguntas  estudiaremos si realmente existe una gran diferencia entre los distintos puestos relacionados a los datos o si realmente existe un desorden en los nombres de cada cargo, ya que realmente las habilidades son distintas pero con otros nombre, es decir averiguaremos si un data engineer es lo mismo que un data scientist o big data analyst, data quality, o por el contrario cuales son sus diferencias.


Para el siguiente ejercicio vamos a analizar datos relacionados entre el rating de las empresas, los salarios y las recomendaciones de los empleados a sus conocidos o amigos.

```{r}
#Lo primero que se realiza es comprobar que datos dentro del dataSet "glassdoor_data" se encuentran en blanco o inexistentes 
sapply(tablaEmpresa, function(x) sum(is.na(x)))


```
```{r}

cantidadNulosRecomendaciones <- sum(is.na(tablaEmpresa$rating.recommendToFriend))
cantidadNulosSalario <- sum(is.na(tablaEmpresa$salary.salaries))
cantidadNulosRating <- sum(is.na(tablaEmpresa$rating.starRating))
cat("La cantidad de datos que se encuentran nulos son:", cantidadNulosRecomendaciones+cantidadNulosSalario+cantidadNulosRating, "datos")

```
```{r}
tablaEmpresaCorregida = na.omit(tablaEmpresa)

#Luego calculamos la media aritmetica para asignarlo a los valores NA del dataSet original 
mediaFriend = round(mean(tablaEmpresaCorregida$rating.recommendToFriend))
mediaRating = round(mean(tablaEmpresaCorregida$rating.starRating))
mediaSalary = round(mean(tablaEmpresaCorregida$salary.salaries))

# asiganamos el calculo de la media aritmetica correspondiente al valor de cada columna
tablaEmpresa$rating.recommendToFriend[is.na(tablaEmpresa$rating.recommendToFriend)] = mediaFriend 
tablaEmpresa$rating.starRating[is.na(tablaEmpresa$rating.starRating)] = mediaRating
tablaEmpresa$salary.salaries[is.na(tablaEmpresa$salary.salaries)] = mediaFriend

```

```{r}

# Para finalizar verificamos que ya no existan datos perdidos:
sapply(tablaEmpresa, function(x) sum(is.na(x)))


```

```{r}

# filtramos el dataset y consideramos solamente los atributos numéricos.
columnasNumericas <- tablaEmpresa[,unlist(lapply(tablaEmpresa, is.numeric))]
head(columnasNumericas,10)

```

```{r}
#Se crea una matriz de correlación a partir de las columnas numéricas.
matrizCorrelacion <- cor(columnasNumericas)
matrizCorrelacion
```

```{r}
#Se obtienen los valores de correlación correspondientes solo al atributo Survived y se muestran.
correlacionBeneficios <- round(matrizCorrelacion["rating.starRating", ], 2)
correlacionBeneficios
```

Como podemos observar en la matriz de correlación los elementos que son tendencias a la hora de defir la categoria o ranking de una empresa informática, tiene estrecha relación con recomendación de amigos, de esto se puede inferir que puede ser una atraves de una charla,red social, etc.Tambien es importante recalcar el prestigio que tiene esta por medio de su año de fundación.


Para el seguiente ejercicio exploraremos los sectores operativos de las distintas empresas, para ver los 10 sectores que más demandan trabajos de tecnología:

```{r}
# Cantidad de industrias distintas que se registraron en los trabajos
length(unique(glassdoor_data$overview.industry))

# tratamiento para transformar las industras con cadenas en blanco "" en valores NA
glassdoor_data$overview.industry[glassdoor_data$overview.industry == ""] <- NA

# Se remueven los valores NA en el atributo industria
industriasSinNulos <- glassdoor_data[!is.na(glassdoor_data$overview.industry),]
nrow(industriasSinNulos)

contadorIndustrias <- data.frame(industriasSinNulos$overview.industry)
sort(table(contadorIndustrias), decreasing = TRUE)[1:10]

```

A continuación averiguaremos las valoraciones que dan los profesionales a las empresas donde han trabajado, esto lo haremos mediante un plot que nos permita visualizar los outliers e intentar eliminar valores que pudiesen dar los trabajadores sin hacer una valoración constructiva, es decir, el trabajador estaba enojado y le puso un 1.0 a la empresa. Esto haría que no pudiesemos ver un promedio de las valoraciones reales.

```{r}

glassdoor_data$rating.starRating[glassdoor_data$rating.starRating == ""] <- NA
glassdoor_data$overview.type[glassdoor_data$overview.type == ""] <- NA

#sector se refiere al sector privado o serctor público
sectorSinNulos <- glassdoor_data[!is.na(glassdoor_data$overview.type),]
ratingsSinNulos <- sectorSinNulos[!is.na(sectorSinNulos$rating.starRating),]

sectores <- filter(ratingsSinNulos, overview.type == "Company - Public" | overview.type == "Company - Private")

plot(sectores$overview.type, sectores$rating.starRating, ylim=c(0,5))
```
Los resultados obtenios es que las compañias privadas tienen una mayor cantidad de valoraciones que las publicas, sin embargos ambas estan a la par en el rating, consiguiendo una media de 3.6.


# Preguntas y problemas

- Es necesaria la estandarización de los datos para realizar un análisis más preciso  y conciso en relación a las características más relevantes que contiene el set de datos?

- Existe la posibilidad de realizar un modelo basado en ML para predecir características basadas en salario? ¿Qué tipo de modelo cree usted que sería el adecuado?

- ¿Qué tipo de beneficios crees que aportará la exploración y posterior aprendizaje automático de los datos capturados?

- Basado en el análisis estadístico de los datos se podría afirmar que de acuerdo a la cantidad de empleados de cada empresa, está entrega mejores beneficios de acuerdo a su tamaño?

- ¿Se puede predecir el rango salarial que tendrá un ingeniero de software dentro de los siguientes años?

- Se pueden crear modelos que midan la tasa de cambio o incorporación de las nuevas habilidades que les van a solicitando a los postulantes. Es sabido que en tecnología están constantemente saliendo nuevas tecnologías por lo que es todo muy cambiante, por lo que hacer un modelo que mida esta tasa de aparición o cambio de las nuevas tecnologías sería una métrica o modelo muy interesante de estudiar.

# Contribuciones

**Rodrigro Pardo**
- Elaboración de la problematica y la motivación junto a Juan Aguilera.
- En la sección de exploración de datos realizó el filtrado de los cargos que más se solicitan.
- En la sección de industrias que más empleos generan en el mundo de la tecnología
- Rango de fechas de los trabajos analizados
- Análisis de los trabajos relacionados con datos y su granularidad.


**Jonathan Lizama**
- Elaboración de un gráfico de los puestos de trabajo más buscados vs la cantidad que estos se buscaron en el dataset.
- Elaboración de las preguntas.
- Elaboración de correlaciones de datos en relación al ranking de empresas.
- Ordenar descendentemente los cargos más demandados

**Juan Aguilera**
- Junto a Rodrigo elaboró la problematica y la motivación
- En la expedición de datos elaboró la dimensión de del dataset que se utilizó.
- Simplificó la visualización de algunas de las columnas más importantes.
- Busco los salarios que aparecen en el dataset para saber cuales son los valores extremos y así poder sacar la diferencia.

enlace: https://github.com/RodrigoPardo19/glassdoor-datamining
